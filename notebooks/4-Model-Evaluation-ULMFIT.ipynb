{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "This model is tested on a much reliable test dataset.\n",
    "\n",
    "Credits to all AA ASL 2018 members who helped in annotating the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/human_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Human Label'].value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index=str, columns={\"Human Label\": \"label\"}, inplace=True)\n",
    "df.rename(index=str, columns={\"cleansed_comment\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.label != 'Neutral'] # filter out Neutral, it is not in our model\n",
    "df.label = df.label.apply(lambda x: x.lower())\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/Users/jiajunkoh/projects/aa-nps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, 'models/awd_lstm_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = [\n",
    "    'text',\n",
    "    'label',\n",
    "    'prob_neg',\n",
    "    'prob_pos',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1557\n",
      "100 1557\n",
      "200 1557\n",
      "300 1557\n",
      "400 1557\n",
      "500 1557\n",
      "600 1557\n",
      "700 1557\n",
      "800 1557\n",
      "900 1557\n",
      "1000 1557\n",
      "1100 1557\n",
      "1200 1557\n",
      "1300 1557\n",
      "1400 1557\n",
      "1500 1557\n"
     ]
    }
   ],
   "source": [
    "# @TODO: Wrap this into batch predict\n",
    "correct_label, wrong_label = 0, 0\n",
    "metrics = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}\n",
    "mis_clas = []\n",
    "with open('../metrics/metrics.csv', 'w') as f:\n",
    "    f.write(','.join(COLS))\n",
    "    f.write('\\n')\n",
    "    for ind, row in df.iterrows():\n",
    "        if (ind % 100 == 0):\n",
    "            print(ind, df.shape[0]) # 'progress bar' lul\n",
    "        p = learn.predict(row.text)\n",
    "        actual = 1 if row.label == 'positive' else 0 # it's easier to deal with number\n",
    "        if p[1] == actual:\n",
    "            correct_label += 1\n",
    "            if p[1] == 1:\n",
    "                metrics['tp'] += 1\n",
    "            else:\n",
    "                metrics['tn'] += 1\n",
    "        else:\n",
    "            wrong_label += 1\n",
    "            # lets look into the mis-classified items\n",
    "            mis_clas.append([row.text, p[2][0].item(), p[2][1].item()])\n",
    "            if p[1] == 1:\n",
    "                metrics['fp'] += 1\n",
    "            else:\n",
    "                metrics['fn'] += 1\n",
    "        # NOTE \n",
    "        # !! p[2][0] - Neg, p[2][1] - Pos !!\n",
    "        csv_row = [row.text, row.label, str(p[2][0].item()), str(p[2][1].item())]\n",
    "        f.write(','.join(csv_row))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1508, 49)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_label, wrong_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.968529222864483, 0.9804772234273319, 0.9186991869918699, 0.9485834207764953)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (metrics['tp'] + metrics['tn'])/(metrics['tp'] + metrics['fp'] + metrics['tn'] + metrics['fn'])\n",
    "precision = metrics['tp']/(metrics['tp'] + metrics['fp'])\n",
    "recall = metrics['tp']/(metrics['tp'] + metrics['fn'])\n",
    "f1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_clas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
